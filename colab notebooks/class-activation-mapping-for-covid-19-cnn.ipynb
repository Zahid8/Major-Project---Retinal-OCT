{"cells":[{"cell_type":"markdown","metadata":{"papermill":{"duration":0.01436,"end_time":"2020-08-11T23:48:10.074460","exception":false,"start_time":"2020-08-11T23:48:10.060100","status":"completed"},"tags":[],"id":"Su7nZqzX9wFy"},"source":["# Introduction and Set-up\n","\n","Gradient-weighted class activation mapping is a great way to better understand what's happening in your CNN. It helps visualize what the important parts of your image are for classification.\n","\n","In this notebook, we're going to be using Grad-CAM to see what parts of the image are important for the CNN when classifying between COVID-19 and pneumonia X-rays. From an initial analysis, it may  seem that COVID-19 and pneumonia images will be relatively similar, given than many cases of COVID-19 cause pnuemonia. The Grad-CAM visualizations may help pinpoint what differentiates these classes of images. \n","\n","Before we run our notebook, make sure to change the accelerator to TPU for quick training."]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2020-08-11T23:48:10.111710Z","iopub.status.busy":"2020-08-11T23:48:10.110769Z","iopub.status.idle":"2020-08-11T23:48:22.690345Z","shell.execute_reply":"2020-08-11T23:48:22.688446Z"},"papermill":{"duration":12.602288,"end_time":"2020-08-11T23:48:22.690737","exception":false,"start_time":"2020-08-11T23:48:10.088449","status":"completed"},"tags":[],"id":"qjy3ooxv9wF9","executionInfo":{"status":"ok","timestamp":1675271708630,"user_tz":-330,"elapsed":2062,"user":{"displayName":"Zahid Hussain","userId":"14872109210270963710"}}},"outputs":[],"source":["import re\n","import os\n","import random\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2020-08-11T23:48:22.737492Z","iopub.status.busy":"2020-08-11T23:48:22.736501Z","iopub.status.idle":"2020-08-11T23:48:23.132519Z","shell.execute_reply":"2020-08-11T23:48:23.131751Z"},"papermill":{"duration":0.428191,"end_time":"2020-08-11T23:48:23.132668","exception":false,"start_time":"2020-08-11T23:48:22.704477","status":"completed"},"tags":[],"id":"GY_pIN9g9wGE"},"outputs":[],"source":["AUTOTUNE = tf.data.experimental.AUTOTUNE\n","GCS_PATH = KaggleDatasets().get_gcs_path(\"covid19-radiography-database\")\n","BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n","IMAGE_SIZE = [180, 180]"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.011074,"end_time":"2020-08-11T23:48:23.155509","exception":false,"start_time":"2020-08-11T23:48:23.144435","status":"completed"},"tags":[],"id":"bOhqFNSY9wGG"},"source":["# Load the images"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-08-11T23:48:23.667504Z","iopub.status.busy":"2020-08-11T23:48:23.661428Z","iopub.status.idle":"2020-08-11T23:48:23.671276Z","shell.execute_reply":"2020-08-11T23:48:23.670598Z"},"papermill":{"duration":0.504461,"end_time":"2020-08-11T23:48:23.671423","exception":false,"start_time":"2020-08-11T23:48:23.166962","status":"completed"},"tags":[],"id":"K2tYlsiw9wGI"},"outputs":[],"source":["filenames = tf.io.gfile.glob(str(GCS_PATH + '/COVID-19 Radiography Database/COVID-19/*'))\n","filenames.extend(tf.io.gfile.glob(str(GCS_PATH + '/COVID-19 Radiography Database/NORMAL/*')))\n","filenames.extend(tf.io.gfile.glob(str(GCS_PATH + '/COVID-19 Radiography Database/Viral Pneumonia/*')))\n","\n","random.seed(1337)\n","tf.random.set_seed(1337)\n","random.shuffle(filenames)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.011282,"end_time":"2020-08-11T23:48:23.694638","exception":false,"start_time":"2020-08-11T23:48:23.683356","status":"completed"},"tags":[],"id":"SUfMuL3w9wGJ"},"source":["Divide the set into training, validation, and testing sets."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-08-11T23:48:23.730070Z","iopub.status.busy":"2020-08-11T23:48:23.729295Z","iopub.status.idle":"2020-08-11T23:48:23.733424Z","shell.execute_reply":"2020-08-11T23:48:23.732797Z"},"papermill":{"duration":0.026973,"end_time":"2020-08-11T23:48:23.733593","exception":false,"start_time":"2020-08-11T23:48:23.706620","status":"completed"},"tags":[],"id":"FIx10khx9wGK"},"outputs":[],"source":["train_filenames, test_filenames = train_test_split(filenames, test_size=0.1)\n","train_filenames, val_filenames = train_test_split(train_filenames, test_size=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-08-11T23:48:23.767406Z","iopub.status.busy":"2020-08-11T23:48:23.766569Z","iopub.status.idle":"2020-08-11T23:48:23.772524Z","shell.execute_reply":"2020-08-11T23:48:23.773458Z"},"papermill":{"duration":0.028614,"end_time":"2020-08-11T23:48:23.773805","exception":false,"start_time":"2020-08-11T23:48:23.745191","status":"completed"},"tags":[],"id":"W9TnfpGU9wGM","outputId":"c5cd6f7a-d71b-4b04-d2a8-dd7e855cfc0d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Normal images count in training set: 1080\n","COVID-19 images count in training set: 183\n","Pneumonia images count in training set: 1089\n"]}],"source":["COUNT_NORMAL = len([filename for filename in train_filenames if \"NORMAL\" in filename])\n","print(\"Normal images count in training set: \" + str(COUNT_NORMAL))\n","\n","COUNT_COVID = len([filename for filename in train_filenames if \"/COVID-19/\" in filename])\n","print(\"COVID-19 images count in training set: \" + str(COUNT_COVID))\n","\n","COUNT_PNEUMONIA = len([filename for filename in train_filenames if \"Viral\" in filename])\n","print(\"Pneumonia images count in training set: \" + str(COUNT_PNEUMONIA))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-08-11T23:48:23.822608Z","iopub.status.busy":"2020-08-11T23:48:23.821630Z","iopub.status.idle":"2020-08-11T23:48:23.830870Z","shell.execute_reply":"2020-08-11T23:48:23.831457Z"},"papermill":{"duration":0.045494,"end_time":"2020-08-11T23:48:23.831639","exception":false,"start_time":"2020-08-11T23:48:23.786145","status":"completed"},"tags":[],"id":"1aBLMKNJ9wGO"},"outputs":[],"source":["train_list_ds = tf.data.Dataset.from_tensor_slices(train_filenames)\n","val_list_ds = tf.data.Dataset.from_tensor_slices(val_filenames)\n","test_list_ds = tf.data.Dataset.from_tensor_slices(test_filenames)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-08-11T23:48:23.862576Z","iopub.status.busy":"2020-08-11T23:48:23.861516Z","iopub.status.idle":"2020-08-11T23:48:23.869883Z","shell.execute_reply":"2020-08-11T23:48:23.869223Z"},"papermill":{"duration":0.026683,"end_time":"2020-08-11T23:48:23.870020","exception":false,"start_time":"2020-08-11T23:48:23.843337","status":"completed"},"tags":[],"id":"31gkzS5g9wGQ","outputId":"ed93f29e-38ca-4573-956c-9b455914a90c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training images count: 2352\n","Validating images count: 262\n"]}],"source":["TRAIN_IMG_COUNT = tf.data.experimental.cardinality(train_list_ds).numpy()\n","print(\"Training images count: \" + str(TRAIN_IMG_COUNT))\n","\n","VAL_IMG_COUNT = tf.data.experimental.cardinality(val_list_ds).numpy()\n","print(\"Validating images count: \" + str(VAL_IMG_COUNT))"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.011433,"end_time":"2020-08-11T23:48:23.893561","exception":false,"start_time":"2020-08-11T23:48:23.882128","status":"completed"},"tags":[],"id":"RhPXTu059wGR"},"source":["The following functions will help us format our dataset into the necessary (image, label) tuple for easy training. We also one-hot encode our labels (i.e. [1 0 0] means NORMAL)."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-08-11T23:48:23.923634Z","iopub.status.busy":"2020-08-11T23:48:23.922501Z","iopub.status.idle":"2020-08-11T23:48:23.926444Z","shell.execute_reply":"2020-08-11T23:48:23.925672Z"},"papermill":{"duration":0.021114,"end_time":"2020-08-11T23:48:23.926577","exception":false,"start_time":"2020-08-11T23:48:23.905463","status":"completed"},"tags":[],"id":"RXkfsUiF9wGR"},"outputs":[],"source":["CLASSES = ['NORMAL', 'COVID-19', 'Viral Pneumonia']"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-08-11T23:48:23.957969Z","iopub.status.busy":"2020-08-11T23:48:23.956959Z","iopub.status.idle":"2020-08-11T23:48:23.960842Z","shell.execute_reply":"2020-08-11T23:48:23.960011Z"},"papermill":{"duration":0.022232,"end_time":"2020-08-11T23:48:23.960976","exception":false,"start_time":"2020-08-11T23:48:23.938744","status":"completed"},"tags":[],"id":"feYMlxPa9wGT"},"outputs":[],"source":["def get_label(file_path):\n","    # convert the path to a list of path components\n","    parts = tf.strings.split(file_path, os.path.sep)\n","    # The second to last is the class-directory\n","    return parts[-2] == CLASSES"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-08-11T23:48:23.992773Z","iopub.status.busy":"2020-08-11T23:48:23.991805Z","iopub.status.idle":"2020-08-11T23:48:23.995429Z","shell.execute_reply":"2020-08-11T23:48:23.994732Z"},"papermill":{"duration":0.02269,"end_time":"2020-08-11T23:48:23.995562","exception":false,"start_time":"2020-08-11T23:48:23.972872","status":"completed"},"tags":[],"id":"lrhCQcFi9wGU"},"outputs":[],"source":["def decode_img(img):\n","  # convert the compressed string to a 3D uint8 tensor\n","  img = tf.image.decode_png(img, channels=3)\n","  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n","  img = tf.image.convert_image_dtype(img, tf.float32)\n","  # resize the image to the desired size.\n","  return tf.image.resize(img, IMAGE_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-08-11T23:48:24.026503Z","iopub.status.busy":"2020-08-11T23:48:24.025722Z","iopub.status.idle":"2020-08-11T23:48:24.029299Z","shell.execute_reply":"2020-08-11T23:48:24.028524Z"},"papermill":{"duration":0.02171,"end_time":"2020-08-11T23:48:24.029431","exception":false,"start_time":"2020-08-11T23:48:24.007721","status":"completed"},"tags":[],"id":"zaka6jy19wGV"},"outputs":[],"source":["def process_path(file_path):\n","    label = get_label(file_path)\n","    # load the raw data from the file as a string\n","    img = tf.io.read_file(file_path)\n","    img = decode_img(img)\n","    return img, label"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-08-11T23:48:24.061551Z","iopub.status.busy":"2020-08-11T23:48:24.060751Z","iopub.status.idle":"2020-08-11T23:48:24.760296Z","shell.execute_reply":"2020-08-11T23:48:24.761066Z"},"papermill":{"duration":0.719651,"end_time":"2020-08-11T23:48:24.761301","exception":false,"start_time":"2020-08-11T23:48:24.041650","status":"completed"},"tags":[],"id":"WdrJgngr9wGW"},"outputs":[],"source":["train_ds = train_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n","val_ds = val_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n","test_ds = test_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-08-11T23:48:24.798085Z","iopub.status.busy":"2020-08-11T23:48:24.797217Z","iopub.status.idle":"2020-08-11T23:48:24.801099Z","shell.execute_reply":"2020-08-11T23:48:24.801682Z"},"papermill":{"duration":0.026838,"end_time":"2020-08-11T23:48:24.801867","exception":false,"start_time":"2020-08-11T23:48:24.775029","status":"completed"},"tags":[],"id":"IGBpvynO9wGX"},"outputs":[],"source":["def prepare_for_training(ds, cache=True):\n","    # This is a small dataset, only load it once, and keep it in memory.\n","    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n","    # fit in memory.\n","    if cache:\n","        if isinstance(cache, str):\n","            ds = ds.cache(cache)\n","        else:\n","            ds = ds.cache()\n","\n","    ds = ds.shuffle(buffer_size=1000)\n","    ds = ds.batch(BATCH_SIZE)\n","\n","    if cache:\n","        ds = ds.prefetch(buffer_size=AUTOTUNE)\n","\n","    return ds"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-08-11T23:48:24.837184Z","iopub.status.busy":"2020-08-11T23:48:24.836162Z","iopub.status.idle":"2020-08-11T23:48:24.853523Z","shell.execute_reply":"2020-08-11T23:48:24.854155Z"},"papermill":{"duration":0.038944,"end_time":"2020-08-11T23:48:24.854407","exception":false,"start_time":"2020-08-11T23:48:24.815463","status":"completed"},"tags":[],"id":"Y4ST5t3Y9wGY"},"outputs":[],"source":["train_ds = prepare_for_training(train_ds)\n","val_ds = prepare_for_training(val_ds)\n","test_ds = prepare_for_training(test_ds, False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-08-11T23:49:14.392766Z","iopub.status.busy":"2020-08-11T23:49:14.391737Z","iopub.status.idle":"2020-08-11T23:49:22.675697Z","shell.execute_reply":"2020-08-11T23:49:22.676350Z"},"papermill":{"duration":8.308746,"end_time":"2020-08-11T23:49:22.676519","exception":false,"start_time":"2020-08-11T23:49:14.367773","status":"completed"},"tags":[],"id":"ddsFu_hw9wGe"},"outputs":[],"source":["with strategy.scope():\n","    reconstructed_model = keras.models.load_model(\"../input/test-model/xray_model.h5\")\n","    reconstructed_model.pop()\n","    reconstructed_model.add(keras.layers.Dense(3, activation='softmax'))\n","    \n","    METRICS = [\n","        'accuracy',\n","        keras.metrics.Precision(name=\"precision\"),\n","        keras.metrics.Recall(name=\"recall\")\n","    ]\n","    \n","    reconstructed_model.compile(\n","        optimizer=\"adam\",\n","        loss=\"categorical_crossentropy\",\n","        metrics=METRICS,\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-08-11T23:49:22.719444Z","iopub.status.busy":"2020-08-11T23:49:22.718512Z","iopub.status.idle":"2020-08-11T23:51:45.843011Z","shell.execute_reply":"2020-08-11T23:51:45.842191Z"},"papermill":{"duration":143.148314,"end_time":"2020-08-11T23:51:45.843143","exception":false,"start_time":"2020-08-11T23:49:22.694829","status":"completed"},"tags":[],"id":"DABL4r3f9wGf","outputId":"7559205f-4957-414d-a5bc-edf01fde8fc5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","19/19 [==============================] - 60s 3s/step - precision: 0.6982 - loss: 0.8479 - accuracy: 0.6637 - recall: 0.5765 - val_precision: 0.8584 - val_loss: 0.4569 - val_accuracy: 0.8244 - val_recall: 0.7634\n","Epoch 2/20\n","19/19 [==============================] - 2s 94ms/step - precision: 0.8667 - loss: 0.4890 - accuracy: 0.8448 - recall: 0.8155 - val_precision: 0.8867 - val_loss: 0.3025 - val_accuracy: 0.8740 - val_recall: 0.8664\n","Epoch 3/20\n","19/19 [==============================] - 2s 93ms/step - precision: 0.8819 - loss: 0.4007 - accuracy: 0.8686 - recall: 0.8542 - val_precision: 0.9134 - val_loss: 0.3163 - val_accuracy: 0.8969 - val_recall: 0.8855\n","Epoch 4/20\n","19/19 [==============================] - 2s 92ms/step - precision: 0.8925 - loss: 0.3470 - accuracy: 0.8831 - recall: 0.8754 - val_precision: 0.9186 - val_loss: 0.2319 - val_accuracy: 0.9122 - val_recall: 0.9046\n","Epoch 5/20\n","19/19 [==============================] - 2s 91ms/step - precision: 0.9101 - loss: 0.3114 - accuracy: 0.8971 - recall: 0.8869 - val_precision: 0.9380 - val_loss: 0.2096 - val_accuracy: 0.9313 - val_recall: 0.9237\n","Epoch 6/20\n","19/19 [==============================] - 2s 93ms/step - precision: 0.9170 - loss: 0.2937 - accuracy: 0.9048 - recall: 0.8929 - val_precision: 0.9310 - val_loss: 0.2003 - val_accuracy: 0.9275 - val_recall: 0.9275\n","Epoch 7/20\n","19/19 [==============================] - 2s 94ms/step - precision: 0.9116 - loss: 0.2899 - accuracy: 0.9043 - recall: 0.8941 - val_precision: 0.9423 - val_loss: 0.1862 - val_accuracy: 0.9389 - val_recall: 0.9351\n","Epoch 8/20\n","19/19 [==============================] - 2s 89ms/step - precision: 0.9172 - loss: 0.2710 - accuracy: 0.9099 - recall: 0.9001 - val_precision: 0.9618 - val_loss: 0.1313 - val_accuracy: 0.9618 - val_recall: 0.9618\n","Epoch 9/20\n","19/19 [==============================] - 2s 80ms/step - precision: 0.9331 - loss: 0.2274 - accuracy: 0.9230 - recall: 0.9188 - val_precision: 0.9462 - val_loss: 0.1527 - val_accuracy: 0.9427 - val_recall: 0.9389\n","Epoch 10/20\n","19/19 [==============================] - 2s 93ms/step - precision: 0.9301 - loss: 0.2255 - accuracy: 0.9260 - recall: 0.9218 - val_precision: 0.9423 - val_loss: 0.1186 - val_accuracy: 0.9427 - val_recall: 0.9351\n","Epoch 11/20\n","19/19 [==============================] - 2s 111ms/step - precision: 0.9301 - loss: 0.2223 - accuracy: 0.9256 - recall: 0.9226 - val_precision: 0.9693 - val_loss: 0.0957 - val_accuracy: 0.9695 - val_recall: 0.9656\n","Epoch 12/20\n","19/19 [==============================] - 2s 81ms/step - precision: 0.9362 - loss: 0.2175 - accuracy: 0.9298 - recall: 0.9239 - val_precision: 0.9577 - val_loss: 0.1264 - val_accuracy: 0.9542 - val_recall: 0.9504\n","Epoch 13/20\n","19/19 [==============================] - 2s 81ms/step - precision: 0.9342 - loss: 0.2187 - accuracy: 0.9286 - recall: 0.9235 - val_precision: 0.9579 - val_loss: 0.1430 - val_accuracy: 0.9542 - val_recall: 0.9542\n","Epoch 14/20\n","19/19 [==============================] - 2s 81ms/step - precision: 0.9481 - loss: 0.1942 - accuracy: 0.9422 - recall: 0.9396 - val_precision: 0.9693 - val_loss: 0.1228 - val_accuracy: 0.9656 - val_recall: 0.9656\n","Epoch 15/20\n","19/19 [==============================] - 2s 82ms/step - precision: 0.9388 - loss: 0.1874 - accuracy: 0.9354 - recall: 0.9320 - val_precision: 0.9692 - val_loss: 0.1326 - val_accuracy: 0.9656 - val_recall: 0.9618\n","Epoch 16/20\n","19/19 [==============================] - 2s 99ms/step - precision: 0.9357 - loss: 0.2218 - accuracy: 0.9311 - recall: 0.9281 - val_precision: 0.9690 - val_loss: 0.1227 - val_accuracy: 0.9580 - val_recall: 0.9542\n"]}],"source":["history = reconstructed_model.fit(\n","    train_ds,\n","    validation_data=val_ds,\n","    epochs=20,\n","    callbacks=[early_stopping_cb]\n",")"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.034456,"end_time":"2020-08-11T23:51:45.910827","exception":false,"start_time":"2020-08-11T23:51:45.876371","status":"completed"},"tags":[],"id":"gyCVXtcC9wGg"},"source":["# Evaluate our model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2020-08-11T23:51:45.971527Z","iopub.status.busy":"2020-08-11T23:51:45.970353Z","iopub.status.idle":"2020-08-11T23:52:04.415497Z","shell.execute_reply":"2020-08-11T23:52:04.416132Z"},"papermill":{"duration":18.47897,"end_time":"2020-08-11T23:52:04.416322","exception":false,"start_time":"2020-08-11T23:51:45.937352","status":"completed"},"tags":[],"id":"Is-mlJWJ9wGh","outputId":"2f84d9e5-f97f-4fc8-d670-4150a3550473"},"outputs":[{"name":"stdout","output_type":"stream","text":["3/3 [==============================] - 4s 1s/step - precision: 0.9689 - loss: 0.1150 - accuracy: 0.9622 - recall: 0.9622\n"]},{"data":{"text/plain":["{'precision': 0.9688581824302673,\n"," 'loss': 0.11499261111021042,\n"," 'accuracy': 0.962199330329895,\n"," 'recall': 0.962199330329895}"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["reconstructed_model.evaluate(test_ds, return_dict=True)"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.026682,"end_time":"2020-08-11T23:52:04.469859","exception":false,"start_time":"2020-08-11T23:52:04.443177","status":"completed"},"tags":[],"id":"x4vS4aZv9wGi"},"source":["We correctly classify all of our testing images as well! Even though we had a very limited number of images, we could build a great model by loading in a pre-trained model."]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.026558,"end_time":"2020-08-11T23:52:04.523326","exception":false,"start_time":"2020-08-11T23:52:04.496768","status":"completed"},"tags":[],"id":"4Ib8GjVV9wGi"},"source":["# Grad-CAM Setup\n","\n","Check out Keras IO [Grad-CAM Code Example](https://keras.io/examples/vision/grad_cam/) for the original source code.\n","\n","To start, let's first look at the structure of our model."]},{"cell_type":"code","source":["model = keras.models.load_model(\"/content/classify_model4 (1).h5\")"],"metadata":{"id":"84yKq3rl-NbK","executionInfo":{"status":"ok","timestamp":1675272469491,"user_tz":-330,"elapsed":1232,"user":{"displayName":"Zahid Hussain","userId":"14872109210270963710"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2020-08-11T23:52:04.584771Z","iopub.status.busy":"2020-08-11T23:52:04.581956Z","iopub.status.idle":"2020-08-11T23:52:04.596910Z","shell.execute_reply":"2020-08-11T23:52:04.595942Z"},"papermill":{"duration":0.046834,"end_time":"2020-08-11T23:52:04.597107","exception":false,"start_time":"2020-08-11T23:52:04.550273","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"pVfrmDiI9wGi","executionInfo":{"status":"ok","timestamp":1675272473580,"user_tz":-330,"elapsed":70,"user":{"displayName":"Zahid Hussain","userId":"14872109210270963710"}},"outputId":"3d70887c-6587-42a8-ae72-6e2b23e32355"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 178, 178, 100)     2800      \n","                                                                 \n"," batch_normalization (BatchN  (None, 178, 178, 100)    400       \n"," ormalization)                                                   \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 59, 59, 100)      0         \n"," )                                                               \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 57, 57, 70)        63070     \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 57, 57, 70)       280       \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 28, 28, 70)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 26, 26, 50)        31550     \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 26, 26, 50)       200       \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 13, 13, 50)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 11, 11, 20)        9020      \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 11, 11, 20)       80        \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 5, 5, 20)         0         \n"," 2D)                                                             \n","                                                                 \n"," dropout (Dropout)           (None, 5, 5, 20)          0         \n","                                                                 \n"," flatten (Flatten)           (None, 500)               0         \n","                                                                 \n"," dense (Dense)               (None, 4)                 2004      \n","                                                                 \n","=================================================================\n","Total params: 109,404\n","Trainable params: 108,924\n","Non-trainable params: 480\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.027295,"end_time":"2020-08-11T23:52:04.654727","exception":false,"start_time":"2020-08-11T23:52:04.627432","status":"completed"},"tags":[],"id":"9OddpJfd9wGk"},"source":["We need to get the output of the last convolution layer. Because I used Sequential instead of Functional API for my pneumonia model in my other notebook, the blocks that I used show us as nested Sequential models instead of as individual layers. That's not a problem because we can look at the structures of internal models as well."]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2020-08-11T23:52:04.716712Z","iopub.status.busy":"2020-08-11T23:52:04.715623Z","iopub.status.idle":"2020-08-11T23:52:04.719071Z","shell.execute_reply":"2020-08-11T23:52:04.719776Z"},"papermill":{"duration":0.037254,"end_time":"2020-08-11T23:52:04.719932","exception":false,"start_time":"2020-08-11T23:52:04.682678","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":190},"id":"q8u9pChn9wGk","executionInfo":{"status":"error","timestamp":1675272512403,"user_tz":-330,"elapsed":50,"user":{"displayName":"Zahid Hussain","userId":"14872109210270963710"}},"outputId":"16bd92ca-3588-43a5-950f-38d5229c98bb"},"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-bb71284b2e1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# last convolution block of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'Dense' object has no attribute 'layers'"]}],"source":["# last convolution block of the model\n","model.layers[14].layers"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.027312,"end_time":"2020-08-11T23:52:04.774745","exception":false,"start_time":"2020-08-11T23:52:04.747433","status":"completed"},"tags":[],"id":"40OAbiV09wGm"},"source":["Define the function to get a NumPy repressentation of our image."]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2020-08-11T23:52:04.837959Z","iopub.status.busy":"2020-08-11T23:52:04.836909Z","iopub.status.idle":"2020-08-11T23:52:04.840305Z","shell.execute_reply":"2020-08-11T23:52:04.839581Z"},"papermill":{"duration":0.037611,"end_time":"2020-08-11T23:52:04.840431","exception":false,"start_time":"2020-08-11T23:52:04.802820","status":"completed"},"tags":[],"id":"WPNWZ5mL9wGm","executionInfo":{"status":"ok","timestamp":1675271998559,"user_tz":-330,"elapsed":427,"user":{"displayName":"Zahid Hussain","userId":"14872109210270963710"}}},"outputs":[],"source":["def get_img_array(img_path, size=180):\n","    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n","    # `array` is a float32 NumPy array\n","    array = keras.preprocessing.image.img_to_array(img)\n","    # We add a dimension to transform our array into a \"batch\"\n","    # of size (1, 180, 180, 3)\n","    array = np.expand_dims(array, axis=0) / 255.0\n","    return array"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.02675,"end_time":"2020-08-11T23:52:04.894348","exception":false,"start_time":"2020-08-11T23:52:04.867598","status":"completed"},"tags":[],"id":"PzZ5LQRw9wGn"},"source":["# Make grad-CAM heatmap\n","\n","Let's define our grad-CAM function. We need to identify our last convolution layer. For our model, this would be the convolution layer within our `sequential_3` model in our summary above. Since we only need the outputs of the layer and not the actual layer itself, we can specify `sequential_3`, our 7th layer in oour model, as the last convolution layer.\n","\n","We also need to specifiy our classifying layers. The flatten layer and the layers that follow it does the classifying for us so we'll label `layers[-5:]` as our classifying layers."]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2020-08-11T23:52:04.965286Z","iopub.status.busy":"2020-08-11T23:52:04.964130Z","iopub.status.idle":"2020-08-11T23:52:04.967078Z","shell.execute_reply":"2020-08-11T23:52:04.967620Z"},"papermill":{"duration":0.046414,"end_time":"2020-08-11T23:52:04.967779","exception":false,"start_time":"2020-08-11T23:52:04.921365","status":"completed"},"tags":[],"id":"yyHDgYpq9wGo","executionInfo":{"status":"ok","timestamp":1675272002608,"user_tz":-330,"elapsed":15,"user":{"displayName":"Zahid Hussain","userId":"14872109210270963710"}}},"outputs":[],"source":["def make_gradcam_heatmap(img_array, model):\n","    # First, we create a model that maps the input image to the activations\n","    # of the last conv layer\n","    last_conv_layer = model.layers[7]\n","    last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)\n","    \n","    # Mark the classifying layers\n","    classifier_layers = model.layers[-5:]\n","\n","    # Second, we create a model that maps the activations of the last conv\n","    # layer to the final class predictions\n","    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n","    x = classifier_input\n","    for classifier_layer in classifier_layers:\n","        x = classifier_layer(x)\n","    classifier_model = keras.Model(classifier_input, x)\n","\n","    # Then, we compute the gradient of the top predicted class for our input image\n","    # with respect to the activations of the last conv layer\n","    with tf.GradientTape() as tape:\n","        # Compute activations of the last conv layer and make the tape watch it\n","        last_conv_layer_output = last_conv_layer_model(img_array)\n","        tape.watch(last_conv_layer_output)\n","        # Compute class predictions\n","        preds = classifier_model(last_conv_layer_output)\n","        top_pred_index = tf.argmax(preds[0])\n","        top_class_channel = preds[:, top_pred_index]\n","\n","    # This is the gradient of the top predicted class with regard to\n","    # the output feature map of the last conv layer\n","    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n","\n","    # This is a vector where each entry is the mean intensity of the gradient\n","    # over a specific feature map channel\n","    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n","\n","    # We multiply each channel in the feature map array\n","    # by \"how important this channel is\" with regard to the top predicted class\n","    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n","    pooled_grads = pooled_grads.numpy()\n","    for i in range(pooled_grads.shape[-1]):\n","        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n","\n","    # The channel-wise mean of the resulting feature map\n","    # is our heatmap of class activation\n","    heatmap = np.mean(last_conv_layer_output, axis=-1)\n","\n","    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n","    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n","    return heatmap"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.026844,"end_time":"2020-08-11T23:52:05.021908","exception":false,"start_time":"2020-08-11T23:52:04.995064","status":"completed"},"tags":[],"id":"4w7N-UY99wGo"},"source":["# Define superimposing function\n","\n","Let's superimpose the heatmap on the original image to visualize what the CNN marks as important and is used to classify the image."]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2020-08-11T23:52:05.088370Z","iopub.status.busy":"2020-08-11T23:52:05.087301Z","iopub.status.idle":"2020-08-11T23:52:05.090052Z","shell.execute_reply":"2020-08-11T23:52:05.090595Z"},"papermill":{"duration":0.041864,"end_time":"2020-08-11T23:52:05.090756","exception":false,"start_time":"2020-08-11T23:52:05.048892","status":"completed"},"tags":[],"id":"cLEESOot9wGp","executionInfo":{"status":"ok","timestamp":1675272006048,"user_tz":-330,"elapsed":20,"user":{"displayName":"Zahid Hussain","userId":"14872109210270963710"}}},"outputs":[],"source":["def superimposed_cam(file_path):\n","    # Prepare image\n","    img_array = get_img_array(file_path)\n","\n","    # Generate class activation heatmap\n","    heatmap = make_gradcam_heatmap(\n","        img_array, reconstructed_model\n","    )\n","\n","    # Rescale the original image\n","    img = img_array * 255\n","\n","    # We rescale heatmap to a range 0-255\n","    heatmap = np.uint8(255 * heatmap)\n","\n","    # We use jet colormap to colorize heatmap\n","    jet = cm.get_cmap(\"jet\")\n","\n","    # We use RGB values of the colormap\n","    jet_colors = jet(np.arange(256))[:, :3]\n","    jet_heatmap = jet_colors[heatmap]\n","\n","    # We create an image with RGB colorized heatmap\n","    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n","    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n","    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n","\n","    # Superimpose the heatmap on original image\n","    superimposed_img = jet_heatmap * 0.4 + img\n","    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img[0])\n","    \n","    return superimposed_img, CLASSES[np.argmax(reconstructed_model.predict(img_array))]"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.026872,"end_time":"2020-08-11T23:52:05.144938","exception":false,"start_time":"2020-08-11T23:52:05.118066","status":"completed"},"tags":[],"id":"rtGEAfKz9wGq"},"source":["# Visualize class activation mapping\n","\n","Let's compare what the model uses to classify COVID-19 X-rays versus Pneumonia X-rays."]},{"cell_type":"code","source":["!pip install --upgrade -q kaggle\n","\n","!mkdir /root/.kaggle\n","import json\n","token = {\n","    \"username\": \"zahidhussain909\",\n","    \"key\": \"39a06efd89d0f2a699143b8d3d62b216\"\n","}\n","\n","with open('/root/.kaggle/kaggle.json', 'w') as config_file:\n","    json.dump(token, config_file)\n","!chmod 600 /root/.kaggle/kaggle.json\n","\n","!kaggle datasets download -d zahidhussain909/denoised-oct-balanced\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9mzuCQ2g_3SM","executionInfo":{"status":"ok","timestamp":1675272184988,"user_tz":-330,"elapsed":13270,"user":{"displayName":"Zahid Hussain","userId":"14872109210270963710"}},"outputId":"c07ae9a3-c042-4144-9359-fbc81b93b7e2"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading denoised-oct-balanced.zip to /content\n"," 99% 1.04G/1.05G [00:08<00:00, 131MB/s]\n","100% 1.05G/1.05G [00:08<00:00, 134MB/s]\n"]}]},{"cell_type":"code","source":["import zipfile\n","zipref=zipfile.ZipFile(\"/content/denoised-oct-balanced.zip\",'r')\n","zipref.extractall()\n","zipref.close()\n","\n","!rm -rf /content/denoised-oct-balanced.zip"],"metadata":{"id":"8IGd1riPAKGi","executionInfo":{"status":"ok","timestamp":1675272248347,"user_tz":-330,"elapsed":22314,"user":{"displayName":"Zahid Hussain","userId":"14872109210270963710"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2020-08-11T23:52:05.317518Z","iopub.status.busy":"2020-08-11T23:52:05.316434Z","iopub.status.idle":"2020-08-11T23:52:05.320098Z","shell.execute_reply":"2020-08-11T23:52:05.319354Z"},"papermill":{"duration":0.14785,"end_time":"2020-08-11T23:52:05.320221","exception":false,"start_time":"2020-08-11T23:52:05.172371","status":"completed"},"tags":[],"id":"YBZdouYz9wGq","executionInfo":{"status":"ok","timestamp":1675272302997,"user_tz":-330,"elapsed":945,"user":{"displayName":"Zahid Hussain","userId":"14872109210270963710"}}},"outputs":[],"source":["covid_filenames = tf.io.gfile.glob('/content/DENOISED OCT/train/CNV/*')\n","pneumonia_filenames = tf.io.gfile.glob('/content/DENOISED OCT/train/NORMAL/*')"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2020-08-11T23:52:05.393082Z","iopub.status.busy":"2020-08-11T23:52:05.384785Z","iopub.status.idle":"2020-08-11T23:52:28.631308Z","shell.execute_reply":"2020-08-11T23:52:28.631871Z"},"papermill":{"duration":23.284062,"end_time":"2020-08-11T23:52:28.632029","exception":false,"start_time":"2020-08-11T23:52:05.347967","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/","height":378},"id":"VLQrj-GC9wGu","executionInfo":{"status":"error","timestamp":1675272399002,"user_tz":-330,"elapsed":568,"user":{"displayName":"Zahid Hussain","userId":"14872109210270963710"}},"outputId":"e2cf663e-a466-47af-99b7-bafacd41ad27"},"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-69898f2128bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuperimposed_cam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/DENOISED OCT/train/NORMAL/10.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-cbafaf83ae0c>\u001b[0m in \u001b[0;36msuperimposed_cam\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msuperimposed_cam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Prepare image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mimg_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_img_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Generate class activation heatmap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-0793d159ea83>\u001b[0m in \u001b[0;36mget_img_array\u001b[0;34m(img_path, size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_img_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m180\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m# `array` is a float32 NumPy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# We add a dimension to transform our array into a \"batch\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/image_utils.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'color_mode must be \"grayscale\", \"rgb\", or \"rgba\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtarget_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m     \u001b[0mwidth_height_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mwidth_height_tuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minterpolation\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_PIL_INTERPOLATION_METHODS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"]}],"source":["\n","\n","img, pred = superimposed_cam('/content/DENOISED OCT/train/NORMAL/10.png')\n","plt.imshow(img)\n","plt.title(pred)\n","plt.axis(\"off\")\n"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.037831,"end_time":"2020-08-11T23:52:28.707767","exception":false,"start_time":"2020-08-11T23:52:28.669936","status":"completed"},"tags":[],"id":"stfng6sh9wGv"},"source":["The top two rows are COVID-19 images and the botton two rows are Pneumonia images. Parts of the image that are redder on the rainbow spectrum are the more \"important\" parts of the image, as defined by the CNN, and purple parts of the image are less important.\n","\n","For this set of 10 COVID-19 images, it seems that our model focuses on one lung more than the other. This may be biologically significant, or it may be an artificial artifact of the model. One of the hardest aspects of computational biology is maneuvering the differences between biological vs computational significance. Having collaborative discussions with experts in both fields will help answer some of these questions. For the Pnueumonia images, at least for the 10 images shown here, it seems that images are seen in a more holistic manner than the COVID-19 images by the model. However, we've only displayed 20 images here. Looking at the rest of the images will give us a clearer picture. "]},{"cell_type":"code","execution_count":null,"metadata":{"papermill":{"duration":0.037334,"end_time":"2020-08-11T23:52:28.782532","exception":false,"start_time":"2020-08-11T23:52:28.745198","status":"completed"},"tags":[],"id":"8_5sDqQr9wGw"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"papermill":{"duration":264.258823,"end_time":"2020-08-11T23:52:28.929297","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2020-08-11T23:48:04.670474","version":"2.1.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}